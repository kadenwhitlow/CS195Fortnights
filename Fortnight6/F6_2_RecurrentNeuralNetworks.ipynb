{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Recurrent Neural Networks\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_2_RecurrentNeuralNetworks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5mRotyt0gI"
      },
      "source": [
        "## Announcement Update\n",
        "\n",
        "AI - English Faculty Candidate: Gabriel Ford\n",
        "\n",
        "Scholarly Presentation: Friday at 9:00am in Howard 309"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNzq_lPkt0gJ"
      },
      "source": [
        "## Reference\n",
        "\n",
        "SLP: RNNs and LSTMs, Chapter 9 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
        "\n",
        "Keras documentation for SimpleRNN Layer: https://keras.io/api/layers/recurrent_layers/simple_rnn/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfPZdrVjt0gK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258c2c17-b236-4910-b7c3-e862363a9394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets keras tensorflow transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhQUWxdWt0gQ"
      },
      "source": [
        "## Recurrent Neural Networks (RNN)\n",
        "\n",
        "A **recurrent neural network** is a neural network with a loop inside of it - some of the outputs in one layer become inputs of the same or an earlier layer\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/F23-CS195NLP/blob/main/images/RNN_highlevel.png?raw=1\">\n",
        "</div>\n",
        "\n",
        "* $x_{t}$: neural network input at time $t$\n",
        "* $h_{t}$: hidden layer state at time $t$\n",
        "* $y_{t}$: output layer state at time $t$\n",
        "\n",
        "*Allows information from past inputs to affect current predictions*\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.1, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1zWLdHAt0gQ"
      },
      "source": [
        "## RNN visualized as a feedforward network\n",
        "\n",
        "In this image, the inputs are shown on bottom and the outputs on top\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/F23-CS195NLP/blob/main/images/RNN_as_feedforward.png?raw=1\" width=400>\n",
        "</div>\n",
        "\n",
        "* $h_{t-1}$: hidden layer state at time $t-1$ is an input to $h_{t}$\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.2, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymASG3cyt0gR"
      },
      "source": [
        "## RNN \"unrolled\" in time\n",
        "\n",
        "Later outputs continue to be influenced by the entire sequence\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/F23-CS195NLP/blob/main/images/RNN_unroll.png?raw=1\" width=500>\n",
        "</div>\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.4, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zllKxIYmt0gS"
      },
      "source": [
        "## Coding up a simple RNN in Keras\n",
        "\n",
        "Defining a Recurrent layer is similar to defining a Dense layer\n",
        "\n",
        "`return_sequences=False` for now, we don't want to return the entire sequence, just the last output\n",
        "\n",
        "`stateful=False` allows the state from one **batch** to carry over to the next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4XdIZUMt0gS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "# A feedforward network with one hidden layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation=\"relu\"))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "# A recurrent network with one layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
        "model.add(SimpleRNN(100,return_sequences=False,stateful=False))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKnPwce1t0gT"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Copy in your code from the non-recurrent neural language model from last time, and replace the Flatten+Dense layer with a SimpleRNN layer like above.\n",
        "* Use the same dataset, `ag_news`, prepared in the same way\n",
        "* Run it with small enough subset to train within a few minutes\n",
        "\n",
        "How do the performances compare?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f51ELzQ9t0gU"
      },
      "source": [
        "## Reducing your context window\n",
        "\n",
        "Because of the sequential nature of the RNN layer, you don't need to pass in as big of a context window.\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/F23-CS195NLP/blob/main/images/RNN_context_simplification.png?raw=1\" width=500>\n",
        "</div>\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.5, https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItFGOpEWt0gU"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/F23-CS195NLP/blob/main/images/RNN_languagemodeling.png?raw=1\" width=700>\n",
        "</div>\n",
        "\n",
        "### Exercise\n",
        "\n",
        "Reduce your `sequence_length` to 1. Train and test again.\n",
        "\n",
        "How do the results compare?\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.6, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pYh9LuNt0gV"
      },
      "source": [
        "## Generating Text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Our Keras RNN-based neural language model doesn't do a great job of generating text\n",
        "\n",
        "### Exercise:\n",
        "\n",
        "Try it with this text generation code from last time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xws2LVnTt0gV",
        "outputId": "93638ee6-72eb-46c2-cfa4-eaf9752fde24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-986e4e6f2148>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstarter_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstarter_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "starter_string = \"the\"\n",
        "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
        "tokens = tokens_list[0]\n",
        "\n",
        "for i in range(50):\n",
        "    curr_seq = tokens[-sequence_length:]\n",
        "    curr_array = np.array([curr_seq])\n",
        "    predicted_probabilities = model.predict(curr_array,verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probabilities)\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    print(predicted_word+\" \",end=\"\")\n",
        "    tokens.append(predicted_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE9MQlVWt0gZ"
      },
      "source": [
        "**One problem:** Keras will reset the state every time you make a call to `model.predict` so we lose the benefit of recurrence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc9cloyct0gZ"
      },
      "source": [
        "## Exerting more control over when the state gets reset\n",
        "\n",
        "If your model uses the `stateful=True` parameter on the recurrent layer, you get more control over when to reset the state.\n",
        "* Downside: it's more of a pain to train the network like that\n",
        "\n",
        "*A workaround:* create another model with the same architecture except for `stateful` and copy the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4dqkCY7t0gZ"
      },
      "outputs": [],
      "source": [
        "# Create a new model with the same architecture but with stateful RNNs\n",
        "stateful_model = Sequential()\n",
        "stateful_model.add(Embedding(input_dim=vocabulary_size, output_dim=50, batch_input_shape=(1, sequence_length))) #batch size of 1\n",
        "stateful_model.add(SimpleRNN(100,return_sequences=False,stateful=True))\n",
        "stateful_model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "# Load the weights from your trained model\n",
        "stateful_model.set_weights(model.get_weights())\n",
        "\n",
        "# Compile the stateful model (required to make predictions)\n",
        "stateful_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB8rD4RCt0gZ",
        "outputId": "29bb5c69-8ac0-496d-d499-7286c51df9c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first time of the us to the most significant trail blazers on the new york reuters the company a new york reuters a new york reuters a new york reuters a new york reuters a new york reuters a new york reuters a new york reuters a new york reuters "
          ]
        }
      ],
      "source": [
        "starter_string = \"the\"\n",
        "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
        "tokens = tokens_list[0]\n",
        "\n",
        "#do this anytime you want to reset the states - for generating a brand new sequence\n",
        "stateful_model.reset_states()\n",
        "\n",
        "for i in range(50):\n",
        "    curr_seq = tokens[-sequence_length:]\n",
        "    curr_array = np.array([curr_seq])\n",
        "    predicted_probabilities = stateful_model.predict(curr_array,verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probabilities)\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    print(predicted_word+\" \",end=\"\")\n",
        "    tokens.append(predicted_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRvA2W9Rt0gZ"
      },
      "source": [
        "## Training a stateful model\n",
        "\n",
        "Keras makes you work a little harder if you want to train a stateful model from the start\n",
        "* Organize your sequences into batches\n",
        "* All batches need to be the same size (say 32 or 64)\n",
        "\n",
        "Might be appropriate if\n",
        "* You have several long documents\n",
        "* Each document takes multiple batches\n",
        "* You *don't* want to reset states between batches\n",
        "* You *do* want to reset states between documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it0cpbMDt0ga"
      },
      "source": [
        "## Throwback to a data set we worked with previously\n",
        "\n",
        "This example is going to do a couple of things\n",
        "* use The Adventures of Sherlock Holmes corpus we download from Project Gutenberg\n",
        "* use the WordPiece tokenizer from Hugging Face\n",
        "    * I want to keep around things like punctuation which gets removed by the Keras tokenizer\n",
        "    * I want to show you how you can mix different tokenizers with Keras models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcBho2JTt0ga",
        "outputId": "414ba6f2-0901-49c6-e7de-2f08dd590e0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (143245 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a sample of the tokenized text:\n",
            "['tall', ',', 'spare', 'figure', 'pass', 'twice', 'in', 'a', 'dark', 'silhouette', 'against', 'the', 'blind', '.', 'He', 'was', 'pacing', 'the', 'room', 'swiftly']\n",
            "\n",
            "Here's the text's ids\n",
            "[3543, 117, 8608, 2482, 2789, 3059, 1107, 170, 1843, 27316, 1222, 1103, 7198, 119, 1124, 1108, 17218, 1103, 1395, 12476]\n",
            "Vocabulary size:\n",
            "28996\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "response = requests.get(\"https://www.gutenberg.org/files/1661/1661-0.txt\")\n",
        "sherlock_raw_text = response.text\n",
        "\n",
        "sherlock_tokens = tokenizer.tokenize( sherlock_raw_text )\n",
        "\n",
        "sherlock_tokens = sherlock_tokens[:10000] #let's limit the size of the text for this workshop\n",
        "\n",
        "print(\"Here's a sample of the tokenized text:\")\n",
        "print(sherlock_tokens[1000:1020])\n",
        "\n",
        "token_ids = tokenizer.convert_tokens_to_ids(sherlock_tokens )\n",
        "print(\"\\nHere's the text's ids\")\n",
        "print(token_ids[1000:1020])\n",
        "\n",
        "print(\"Vocabulary size:\")\n",
        "print(len(tokenizer.vocab))\n",
        "vocabulary_size = len(tokenizer.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UW-VYErt0gd"
      },
      "source": [
        "### Preparing the list of predictor/target pairs like before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yqaGHvzt0ge"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, SimpleRNN\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import pad_sequences\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "sequence_length = 1\n",
        "batch_size = 32\n",
        "\n",
        "predictor_sequences = []\n",
        "targets = []\n",
        "for i in range(sequence_length, len(token_ids)):\n",
        "    # Take the sequence of tokens as input and the next token as target\n",
        "    curr_target = token_ids[i]\n",
        "    curr_predictor_sequence = token_ids[i-sequence_length:i]\n",
        "    predictor_sequences.append(curr_predictor_sequence)\n",
        "    targets.append(curr_target)\n",
        "\n",
        "# Convert target to one-hot encoding\n",
        "targets_one_hot = to_categorical(targets, num_classes=vocabulary_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y9hOK-Tt0ge"
      },
      "source": [
        "### Grouping the sequences into batches of 32\n",
        "\n",
        "This adds an extra dimension to our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T47x5rJWt0ge",
        "outputId": "7cdee410-4665-4e13-c1e5-9410f99a2d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before batching\n",
            "[[ 261]\n",
            " [ 221]\n",
            " [ 225]\n",
            " ...\n",
            " [1103]\n",
            " [1402]\n",
            " [1167]]\n",
            "\n",
            "after batching\n",
            "[[[  261]\n",
            "  [  221]\n",
            "  [  225]\n",
            "  ...\n",
            "  [ 1329]\n",
            "  [ 1104]\n",
            "  [ 2256]]\n",
            "\n",
            " [[ 5456]\n",
            "  [ 1107]\n",
            "  [ 1103]\n",
            "  ...\n",
            "  [ 1283]\n",
            "  [ 1137]\n",
            "  [ 1231]]\n",
            "\n",
            " [[  118]\n",
            "  [ 1329]\n",
            "  [ 1122]\n",
            "  ...\n",
            "  [ 1409]\n",
            "  [ 1128]\n",
            "  [ 1132]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1332]\n",
            "  [  170]\n",
            "  [ 1590]\n",
            "  ...\n",
            "  [ 6150]\n",
            "  [ 1166]\n",
            "  [26703]]\n",
            "\n",
            " [[18623]\n",
            "  [  117]\n",
            "  [ 1105]\n",
            "  ...\n",
            "  [ 1106]\n",
            "  [ 1143]\n",
            "  [  117]]\n",
            "\n",
            " [[ 1105]\n",
            "  [ 1145]\n",
            "  [ 1107]\n",
            "  ...\n",
            "  [ 1122]\n",
            "  [ 1108]\n",
            "  [ 2330]]]\n"
          ]
        }
      ],
      "source": [
        "def put_into_batches(data,batch_size):\n",
        "    num_batches = (len(data) // batch_size)\n",
        "    batched_data = []\n",
        "    for batch_idx in range(num_batches):\n",
        "        curr_batch = data[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
        "        batched_data.append(curr_batch)\n",
        "    batched_data = np.array(batched_data)\n",
        "    return batched_data\n",
        "\n",
        "\n",
        "train_features_batched = put_into_batches(predictor_sequences,batch_size)\n",
        "train_targets_batched = put_into_batches(targets_one_hot,batch_size)\n",
        "\n",
        "print(\"before batching\")\n",
        "print(np.array(predictor_sequences))\n",
        "\n",
        "print(\"\\nafter batching\")\n",
        "print(train_features_batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSJC0H5dt0gf"
      },
      "source": [
        "## Creating and compiling the model\n",
        "\n",
        "Note that in this case, we set `batch_input_shape=(batch_size, sequence_length)`\n",
        "\n",
        "instead of `input_length=sequence_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOp4X6DRt0gf"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, batch_input_shape=(batch_size, sequence_length)))\n",
        "model.add(SimpleRNN(100,return_sequences=False,stateful=True))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqaAI_JKt0gl"
      },
      "source": [
        "## Writing a training loop\n",
        "\n",
        "instead of just doing `model.fit`, we'll do `model.train_on_batch`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOtg8ucCt0gp",
        "outputId": "820d60cf-8fef-4e4b-b3f0-8e0a7f6ff84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 2/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 3/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 4/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 5/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 6/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 7/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 8/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 9/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 10/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10  # Number of epochs to train for\n",
        "number_of_batches = len(train_features_batched)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    model.reset_states()  # Reset states at the start of each epoch\n",
        "\n",
        "\n",
        "    for batch_idx in range(number_of_batches):\n",
        "        #print batch number every 1000 batches\n",
        "        if (batch_idx+1) % 100 == 0:\n",
        "            print(f'\\tBatch {batch_idx+1}/{number_of_batches}')\n",
        "\n",
        "        # Train on the batch\n",
        "        model.train_on_batch(train_features_batched[batch_idx], train_targets_batched[batch_idx])\n",
        "\n",
        "\n",
        "\n",
        "    # if you switch to a new document, do this\n",
        "    #model.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxy1AXGdt0gt"
      },
      "source": [
        "### Now let's use our model to generate some text\n",
        "\n",
        "This code looks much different because we're using the Hugging Face tokenizer\n",
        "* turn text into ids with `tokenizer.encode`\n",
        "* turn ids into text with `tokenizer.decode`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwiY2t_Ut0gt",
        "outputId": "4c2e090a-d3f2-40c9-b371-783bc183be9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the corner, and theut of the corner, and theut of the corner, and theut of the corner, and theut of the corner, and theut of the corner, and theut of the corner, and theut of the corner\n"
          ]
        }
      ],
      "source": [
        "starter_string = \"the\"\n",
        "\n",
        "# Encode the starter string to token IDs\n",
        "input_ids = tokenizer.encode(starter_string, add_special_tokens=False)\n",
        "\n",
        "for i in range(50):\n",
        "    # Get the last sequence_length tokens\n",
        "    curr_seq = input_ids[-sequence_length:]\n",
        "    # Predict the next token ID\n",
        "    predicted_probabilities = model.predict(np.array([curr_seq]), verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probabilities, axis=-1)\n",
        "    # Add the predicted token ID to the sequence\n",
        "    input_ids.append(predicted_index[0])\n",
        "\n",
        "# Decode the token IDs to a string\n",
        "generated_sequence = tokenizer.decode(input_ids, clean_up_tokenization_spaces=True)\n",
        "print(generated_sequence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRf2QDx2t0gt"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "Adjust the code to get this working on more than one longer document\n",
        "* can get multiple Project Gutenberg texts\n",
        "* can use a Hugging Face dataset with longer texts (i.e., multiple sentences per entry, unlike `ag_news`)\n",
        "\n",
        "Let it train for a while and then generate some text\n",
        "* Did training with larger data sets improve the kind of text you were able to generate?\n",
        "* describe what you did and write up an interpretation of your results"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
